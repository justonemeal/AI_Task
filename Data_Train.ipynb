{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsC7sfU-6dnj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import random\n",
        "import glob\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact\n",
        "\n",
        "random_seed = 2022\n",
        "\n",
        "random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed(random_seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/lettuce/train\"\n",
        "\n",
        "def image_rad():\n",
        "\n",
        "  all_image_path = []\n",
        "\n",
        "  all_image_path.extend(glob.glob(data_dir + '/**/*.jpg'))\n",
        "  all_image_path.extend(glob.glob(data_dir + '/**/*.png'))\n",
        "  all_image_path.extend(glob.glob(data_dir + '/**/*.jpeg'))\n",
        "\n",
        "  print(f\"Total images: {len(all_image_path)}\")\n",
        "  print(all_image_path[:5])\n",
        "\n",
        "  return all_image_path\n",
        "\n",
        "def image_path_lbale():\n",
        "  image_path_lable = []\n",
        "\n",
        "  all_image_path = image_rad()\n",
        "\n",
        "  for path in all_image_path:\n",
        "    label = os.path.basename(os.path.dirname(path))\n",
        "    image_path_lable.append((path, label))\n",
        "\n",
        "  return image_path_lable\n",
        "\n",
        "train_data_dir = \"/content/drive/MyDrive/lettuce/train/\"\n",
        "\n",
        "class Chest_dataset(Dataset):\n",
        "  def __init__(self, data_dir, transform=None):\n",
        "    self.files_path = image_path_lbale()\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.files_path)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image_file = self.files_path[index][0]\n",
        "    image = cv2.imread(image_file)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_VGR2GB)\n",
        "\n",
        "    target = self.files_path[index][1]\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "      target = torch.Tensor([target]).long()\n",
        "\n",
        "    return {\"image\": image, \"target\": target}\n",
        "\n",
        "dset = Chest_dataset(train_data_dir)\n",
        "print(len(dset))"
      ],
      "metadata": {
        "id": "iM0CKPuV6gys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                         std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "train_dset = Chest_dataset(train_data_dir, transformer)\n",
        "index = 200\n",
        "image = train_dset[index][\"image\"]\n",
        "label = train_dset[index][\"target\"]\n",
        "\n",
        "print(image.shape, label)"
      ],
      "metadata": {
        "id": "5HnnXZxU6kY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataloader(train_data_dir, val_data_dir):\n",
        "    dataloaders = {}\n",
        "    train_dset = Chest_dataset(train_data_dir, transformer)\n",
        "    dataloaders[\"train\"] = DataLoader(train_dset, batch_size=4, shuffle=True, drop_last=True)\n",
        "    val_dset = Chest_dataset(val_data_dir, transformer)\n",
        "    dataloaders[\"val\"] = DataLoader(val_dset, batch_size=1, shuffle=False, drop_last=False)\n",
        "    return dataloaders"
      ],
      "metadata": {
        "id": "9ReHBPYE6mVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_dir = \"/content/drive/MyDrive/lettuce/train/\"\n",
        "val_data_dir = \"/content/drive/MyDrive/lettuce/test/\"\n",
        "dataloaders = build_dataloader(train_data_dir, val_data_dir)"
      ],
      "metadata": {
        "id": "lvVQtQeC6ott"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InvertedBottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, t, stride = 1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.stride = stride\n",
        "\n",
        "        expand = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels * t, 1, bias = False),\n",
        "            nn.BatchNorm2d(in_channels * t),\n",
        "            nn.ReLU6(inplace = True),\n",
        "        )\n",
        "        depthwise = nn.Sequential(\n",
        "            nn.Conv2d(in_channels * t, in_channels * t, 3, stride = stride, padding = 1, groups = in_channels * t, bias = False),\n",
        "            nn.BatchNorm2d(in_channels * t),\n",
        "            nn.ReLU6(inplace = True),\n",
        "        )\n",
        "        pointwise = nn.Sequential(\n",
        "            nn.Conv2d(in_channels * t, out_channels, 1, bias = False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        )\n",
        "\n",
        "        residual_list = []\n",
        "        if t > 1:\n",
        "            residual_list += [expand]\n",
        "        residual_list += [depthwise, pointwise]\n",
        "        self.residual = nn.Sequential(*residual_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.stride == 1 and self.in_channels == self.out_channels:\n",
        "            out = self.residual(x) + x\n",
        "        else:\n",
        "            out = self.residual(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    def __init__(self, n_classes = 1000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.first_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, stride = 2, padding = 1, bias = False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU6(inplace = True)\n",
        "        )\n",
        "\n",
        "        self.bottlenecks = nn.Sequential(\n",
        "            self.make_stage(32, 16, t = 1, n = 1),\n",
        "            self.make_stage(16, 24, t = 6, n = 2, stride = 2),\n",
        "            self.make_stage(24, 32, t = 6, n = 3, stride = 2),\n",
        "            self.make_stage(32, 64, t = 6, n = 4, stride = 2),\n",
        "            self.make_stage(64, 96, t = 6, n = 3),\n",
        "            self.make_stage(96, 160, t = 6, n = 3, stride = 2),\n",
        "            self.make_stage(160, 320, t = 6, n = 1)\n",
        "        )\n",
        "\n",
        "        self.last_conv = nn.Sequential(\n",
        "            nn.Conv2d(320, 1280, 1, bias = False),\n",
        "            nn.BatchNorm2d(1280),\n",
        "            nn.ReLU6(inplace = True)\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Sequential(\n",
        "        \tnn.Dropout(0.2),\n",
        "            nn.Linear(1280, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.first_conv(x)\n",
        "        x = self.bottlenecks(x)\n",
        "        x = self.last_conv(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1) # (N, C, 1, 1) -> (N, C)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def make_stage(self, in_channels, out_channels, t, n, stride = 1):\n",
        "        layers = [InvertedBottleneck(in_channels, out_channels, t, stride)]\n",
        "        in_channels = out_channels\n",
        "        for _ in range(n-1):\n",
        "            layers.append(InvertedBottleneck(in_channels, out_channels, t))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "model = MobileNetV2()"
      ],
      "metadata": {
        "id": "Dwt-Tx7Z6q7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr= 1E-3, momentum=0.9)\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_accuracy(image, target, model):\n",
        "    batch_size = image.shape[0]\n",
        "    prediction = model(image)\n",
        "    _, pred_label = torch.max(prediction, dim=1)\n",
        "    is_correct = (pred_label == target)\n",
        "    return is_correct.cpu().numpy().sum() / batch_size"
      ],
      "metadata": {
        "id": "9kZB5z796uZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "loss_func = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr= 1E-3, momentum=0.9)"
      ],
      "metadata": {
        "id": "FQB-5Ge46w53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(dataloaders, model, optimizer, loss_func, device):\n",
        "    losses = {}\n",
        "    accuracies = {}\n",
        "    for phase in [\"train\", \"val\"]:\n",
        "        running_loss = 0.0\n",
        "        running_correct = 0\n",
        "\n",
        "        if phase == \"train\":\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "\n",
        "        for index, batch in enumerate(dataloaders[phase]):\n",
        "            image = batch[\"image\"].to(device)\n",
        "            target = batch[\"target\"].to(device).view(-1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.set_grad_enabled(phase == \"train\"):\n",
        "                prediction = model(image)\n",
        "                loss = loss_func(prediction, target)\n",
        "\n",
        "                if phase == \"train\":\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_correct += get_accuracy(image, target, model)\n",
        "\n",
        "            if phase == \"train\" and index % 10 == 0:\n",
        "                print(f\"{index}/{len(dataloaders[phase])} - Running Loss: {loss.item()}\")\n",
        "\n",
        "        losses[phase] = running_loss / len(dataloaders[phase])\n",
        "        accuracies[phase] = running_correct / len(dataloaders[phase])\n",
        "    return losses, accuracies"
      ],
      "metadata": {
        "id": "12a0B_gh62MN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "model = MobileNetV2().to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    losses, accuracies = train_one_epoch(dataloaders, model, optimizer, loss_func, device)\n",
        "    print(f\"{epoch+1}/{num_epochs}-Train Loss: {losses['train']}, Val Loss: {losses['val']}\")\n",
        "    print(f\"{epoch+1}/{num_epochs}-Train Acc: {accuracies['train']}, Val Acc: {accuracies['val']}\")\n",
        "\n",
        "    if accuracies[\"val\"] > best_acc:\n",
        "        best_acc = accuracies[\"val\"]\n",
        "        best_model = copy.deepcopy(model.state_dict())\n",
        "\n",
        "save_best_model(best_model, \"best_model.pth\")\n",
        "print(\"Best model saved successfully.\")"
      ],
      "metadata": {
        "id": "sai3WOlw62xr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}